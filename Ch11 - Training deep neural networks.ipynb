{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training deep neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As problems are more challenging, deeper NNs are used, which are more complex\n",
    "* Potential problems include: vanishing gradients, exploding gradients, insufficient data, slow training, lots of parameters risks overfitting\n",
    "\n",
    "Main problems:\n",
    "\n",
    "1. Vanishing/exploding gradients\n",
    "2. Reusing pretrained layers\n",
    "3. Faster optimisers\n",
    "4. Avoiding overfitting using regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Vanishing/exploding gradients\n",
    "\n",
    "* DNNs use backpropagation, but gradients can get very small on lower layers and so training never converges\n",
    "* Other side is where gradients are very large and training diverges\n",
    "* Previously DNNs used logistic sigmoid activation function and normalized initial weights, which together have more variance than the inputs\n",
    "* When inputs to logistic function are very large negative or positive, this leads to zero gradient and so nothing to propagate \n",
    "* Early 2000s paper argued that the variance of the outputs of an activation layer had to be similar to variance of inputs, or chaining gives extremities\n",
    "* ReLU behaves better in DNNs because it does not saturate for positive values\n",
    "* ReLU can lead to nodes \"dying\" where the gradient is zero and so there are no subsequent updates, so \"leaky\" ReLU is used\n",
    "* LeakyReLU = max(alpha * z, z), where 0 < alpha < 1, e.g. alpha = 0.01, and so there is small negative gradient\n",
    "* Another improvement was from exponential linear unit (ELU) which is similar to LeakyRLU but is smooth\n",
    "* 2017 paper suggested SELU (Scaled ELU), then the network will self-normalize under certain conditions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the LeakyReLU activation function\n",
    "\n",
    "# leaky_relu = keras.layers.LeakyReLU(alpha=0.2)\n",
    "# layer = keras.layers.Dense(10, activation=leaky_relu, kernel_initializer=\"he_normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Batch normalization (BN) - another approach to deal with vanishing/exploding gradients\n",
    "* Add an operation before the activation function, zero-centring and normalizing each input, then shifting the result using that shift and scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Reusing pretrained layers\n",
    "\n",
    "* Generally not a good idea to train a very large DNN from scratch\n",
    "* Reusing the lower layers from a pretrained network: \"transfer learning\"\n",
    "* You'll need to preprocess the input from the new context so it's similar to the pretraining context\n",
    "* To reuse pretrained layers, they are \"frozen\" so the weights are non-trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_A = keras.models.load_model(\"my_model_A.h5\")\n",
    "# model_B_on_A = keras.models.Sequential(model_A.layers[:-1])\n",
    "# model_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "# model_A_clone = keras.models.clone_model(model_A)\n",
    "\n",
    "## ** Note that definining new instance of models, and clone_model BOTH have shared weights between A and B\n",
    "\n",
    "# model_A_clone.set_weights(model_A.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freezing layers - use 'trainable' attribute\n",
    "\n",
    "# for layer in model_B_on_A.layers[:-1]:\n",
    "#   layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Faster optimisers\n",
    "\n",
    "* Potential for big speed gains through different gradient descent optimizer\n",
    "* Momentum optimization includes the vectors from the previous iterations, plus the change at that iteration\n",
    "* A hyperparameter is introduced for friction, otherwise it might overshoot the optimal point and oscillate around it\n",
    "* Nesterov accelerated gradient is similar but uses the new momentum vector as input \n",
    "* AdaGrad scales the vector along the steepest dimensions\n",
    "* RMSProp gets the gradients from the most recent iterations, using exponential decay\n",
    "* Adam (adaptive moment estimation) combines momentum and expontential decay\n",
    "* Learning rate scheduling: possible to estimate best learning rate, then set that, and also have the rate change during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up momentum in keras\n",
    "\n",
    "# optimiser = keras.optimizers.SGD(lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Avoiding overfitting through regularization \n",
    "\n",
    "* l1 and l2 regularisation used, stopping the weights from being too large\n",
    "* dropout : highly successful, at every training step, each neuron except output has a probability p of being \"dropped out\" - ignored during this training step\n",
    "* dropout is typically set 10% - 50%\n",
    "* intuition behind dropout: this stops neurons from being too reliant on particular inputs and developing more flexible functions\n",
    "* dropout models will perform differently on test and train, since there are no dropouts at the testing stage\n",
    "* monte carlo (MC) dropout - stacking matrices of predictions and averaging gives a MC estimate that is generally more reliable than a single prediction from a model trained with dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
